{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标\n",
    "\n",
    "    使用tensorflow中听的inceptionv3，进行图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\PY3.6\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意这里的斜杠需要是正向斜杠 /\n",
    "model_dir='E:/Projects/00_MyLab/12_tensorflow/inception_model'\n",
    "image='E:/Projects/00_MyLab/12_tensorflow/images/husky.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "整个类的目标：\n",
    "\n",
    "    1、读取 分类序号-类别id，以及类别id-类别名称这两个文件，并转化为字典。\n",
    "    2、最后生成 分类序号-类别名称这个字典。\n",
    "    3、提供 ‘使用分类序号查询类别名称’ 功能\n",
    "'''\n",
    "class NodeLookUp(object):\n",
    "    def __init__(self,label_lookup_path=None,uid_lookup_path=None):\n",
    "        if not label_lookup_path:\n",
    "            # 加载label的查询文件，即imagenet_2012_challenge_label_map_proto.pbtxt\n",
    "            label_lookup_path = os.path.join(model_dir,\n",
    "                                             'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "        if not uid_lookup_path:\n",
    "            # 加载label可读化文件，将uid转换为单词句子\n",
    "            uid_lookup_path=os.path.join(model_dir,\n",
    "                                         'imagenet_synset_to_human_label_map.txt')\n",
    "        \n",
    "        # 这里很迷啊，为什么能够在初始化方法里面直接调用下面的方法？\n",
    "        # 我记得java里面认为初始化方法被调用的时候，下面的方法还没有被创建好啊…\n",
    "        self.node_lookup = self.load(label_lookup_path,uid_lookup_path)\n",
    "    \n",
    "    def load(self,label_lookup_path,uid_lookup_path):\n",
    "        if not tf.gfile.Exists(uid_lookup_path):\n",
    "            # 检查地址存在与否\n",
    "            tf.logging.fatal('File does not exist %s',uid_lookup_path)\n",
    "        if not tf.gfile.Exists(label_lookup_path):\n",
    "            tf.logging.fatal('File does not exist %s',label_lookup_path)\n",
    "            \n",
    "        # 读取uid里面所有的line\n",
    "        proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "        \n",
    "        # 创建字典用来存 id-word的映射关系\n",
    "        uid_to_human={}\n",
    "        for line in proto_as_ascii_lines:\n",
    "            line=line.strip('\\n')\n",
    "            parse_items = line.split('\\t')\n",
    "            # 获取编码\n",
    "            uid = parse_items[0]\n",
    "            # 获取名称\n",
    "            human_string=parse_items[1]\n",
    "            # 存入字典\n",
    "            uid_to_human[uid]=human_string\n",
    "            \n",
    "        # 读取分类字符串和uid对应的文件\n",
    "        proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "        # 同样创建字典保存关系\n",
    "        node_id_to_uid={}\n",
    "        for line in proto_as_ascii:\n",
    "            # 按行取内容\n",
    "            if line.startswith('  target_class:'):\n",
    "                target_class = int(line.split(': ')[1])\n",
    "            if line.startswith('  target_class_string:'):\n",
    "                target_class_string = line.split(': ')[1]\n",
    "                node_id_to_uid[target_class]=target_class_string[1:-2]\n",
    "        \n",
    "        # 然后把第二个字典的key，与第一个字典的value在对应起来\n",
    "        node_id_to_name={}\n",
    "        for key,val in node_id_to_uid.items():\n",
    "            if val not in uid_to_human: # 这里是指第二个dict的value，不在第一个dict的key中的话\n",
    "                tf.logging.fatal('Failed to locate: %s',val)\n",
    "            name = uid_to_human[val]\n",
    "            node_id_to_name[key] = name\n",
    "        \n",
    "        return node_id_to_name\n",
    "\n",
    "    def id_to_string(self,node_id):\n",
    "        if node_id  not in self.node_lookup:\n",
    "            return ''\n",
    "        return self.node_lookup[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "需要取了解一下tf存储模型的方式：仅参数，or 参数+图，以及相互的恢复方法\n",
    "\n",
    "这个方法就是恢复模型的方法\n",
    "'''\n",
    "def create_graph():\n",
    "    # 还有这个tf.gfile是个啥\n",
    "    with tf.gfile.FastGFile(os.path.join(model_dir,\n",
    "                                         'classify_image_graph_def.pb'),\n",
    "                            'rb') as f:\n",
    "        graph_def = tf.GraphDef() # 这是个啥\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def,name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = tf.gfile.FastGFile(image,'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eskimo dog, husky (score = 0.61329)\n",
      "Siberian husky (score = 0.22319)\n",
      "dogsled, dog sled, dog sleigh (score = 0.00641)\n",
      "malamute, malemute, Alaskan malamute (score = 0.00371)\n",
      "Norwegian elkhound, elkhound (score = 0.00184)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "问题1：\n",
    "    平常我们跑的都是节点，这里说拿的是张量，这两个有什么区别吗？\n",
    "'''\n",
    "create_graph()\n",
    "with tf.Session() as sess:\n",
    "    # 拿到最后一层输出的张量\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    # 输入图像数据\n",
    "    predictions = sess.run(softmax_tensor,{'DecodeJpeg/contents:0':image_data})\n",
    "    # 将结果转化为1维数据\n",
    "    predictions = np.squeeze(predictions)\n",
    "    # 新建一个刚刚写的id类\n",
    "    node_lookup = NodeLookUp()\n",
    "    # 取出预测值较大的前五个索引\n",
    "    top_5 = predictions.argsort()[-5:][::-1]\n",
    "    for node_id in top_5:\n",
    "        # 获取分类名称\n",
    "        human_string = node_lookup.id_to_string(node_id)\n",
    "        # 获取置信度\n",
    "        score = predictions[node_id]\n",
    "        # 打印结果\n",
    "        print('%s (score = %.5f)' % (human_string, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
