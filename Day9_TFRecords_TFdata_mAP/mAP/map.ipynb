{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import operator\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "MINOVERLAP = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.ignore =[]\n",
    "specific_iou_flagged = False\n",
    "img_path = 'images'\n",
    "show_animation = False\n",
    "draw_plot = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_average_miss_rate(precision, fp_cumsum, num_images):\n",
    "    \"\"\"\n",
    "        log-average miss rate:\n",
    "            Calculated by averaging miss rates at 9 evenly spaced FPPI points\n",
    "            between 10e-2 and 10e0, in log-space.\n",
    "\n",
    "        output:\n",
    "                lamr | log-average miss rate\n",
    "                mr | miss rate\n",
    "                fppi | false positives per image\n",
    "\n",
    "        references:\n",
    "            [1] Dollar, Piotr, et al. \"Pedestrian Detection: An Evaluation of the\n",
    "               State of the Art.\" Pattern Analysis and Machine Intelligence, IEEE\n",
    "               Transactions on 34.4 (2012): 743 - 761.\n",
    "    \"\"\"\n",
    "\n",
    "    # if there were no detections of that class\n",
    "    if precision.size == 0:\n",
    "        lamr = 0\n",
    "        mr = 1\n",
    "        fppi = 0\n",
    "        return lamr, mr, fppi\n",
    "\n",
    "    fppi = fp_cumsum / float(num_images)\n",
    "    mr = (1 - precision)\n",
    "\n",
    "    fppi_tmp = np.insert(fppi, 0, -1.0)\n",
    "    mr_tmp = np.insert(mr, 0, 1.0)\n",
    "\n",
    "    # Use 9 evenly spaced reference points in log-space\n",
    "    ref = np.logspace(-2.0, 0.0, num = 9)\n",
    "    for i, ref_i in enumerate(ref):\n",
    "        # np.where() will always find at least 1 index, since min(ref) = 0.01 and min(fppi_tmp) = -1.0\n",
    "        j = np.where(fppi_tmp <= ref_i)[-1][-1]\n",
    "        ref[i] = mr_tmp[j]\n",
    "\n",
    "    # log(0) is undefined, so we use the np.maximum(1e-10, ref)\n",
    "    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))\n",
    "\n",
    "    return lamr, mr, fppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " throw error and exit\n",
    "\"\"\"\n",
    "def error(msg):\n",
    "    print(msg)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    " check if the number is a float between 0.0 and 1.0\n",
    "\"\"\"\n",
    "def is_float_between_0_and_1(value):\n",
    "    try:\n",
    "        val = float(value)\n",
    "        if val > 0.0 and val < 1.0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Calculate the AP given the recall and precision array\n",
    "    1st) We compute a version of the measured precision/recall curve with\n",
    "         precision monotonically decreasing\n",
    "    2nd) We compute the AP as the area under this curve by numerical integration.\n",
    "\"\"\"\n",
    "def voc_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "            mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "        (goes from the end to the beginning)\n",
    "        matlab: for i=numel(mpre)-1:-1:1\n",
    "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #     range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "        mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    \"\"\"\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "        (numerical integration)\n",
    "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap, mrec, mpre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Convert the lines of a file to a list\n",
    "\"\"\"\n",
    "def file_lines_to_list(path):\n",
    "    # open txt file lines to a list\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Draws text in image\n",
    "\"\"\"\n",
    "def draw_text_in_image(img, text, pos, color, line_width):\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    fontScale = 1\n",
    "    lineType = 1\n",
    "    bottomLeftCornerOfText = pos\n",
    "    cv2.putText(img, text,\n",
    "            bottomLeftCornerOfText,\n",
    "            font,\n",
    "            fontScale,\n",
    "            color,\n",
    "            lineType)\n",
    "    text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n",
    "    return img, (line_width + text_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Plot - adjust axes\n",
    "\"\"\"\n",
    "def adjust_axes(r, t, fig, axes):\n",
    "    # get text width for re-scaling\n",
    "    bb = t.get_window_extent(renderer=r)\n",
    "    text_width_inches = bb.width / fig.dpi\n",
    "    # get axis width in inches\n",
    "    current_fig_width = fig.get_figwidth()\n",
    "    new_fig_width = current_fig_width + text_width_inches\n",
    "    propotion = new_fig_width / current_fig_width\n",
    "    # get axis limit\n",
    "    x_lim = axes.get_xlim()\n",
    "    axes.set_xlim([x_lim[0], x_lim[1]*propotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Draw plot using Matplotlib\n",
    "\"\"\"\n",
    "def draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 这一块就是创建一些文件夹的，用来保存结果\n",
    " Create a \".temp_files/\" and \"results/\" directory\n",
    "\"\"\"\n",
    "TEMP_FILES_PATH = \".temp_files\"\n",
    "if not os.path.exists(TEMP_FILES_PATH): # if it doesn't exist already\n",
    "    os.makedirs(TEMP_FILES_PATH)\n",
    "results_files_path = \"results\"\n",
    "if os.path.exists(results_files_path): # if it exist already\n",
    "    # reset the results directory\n",
    "    shutil.rmtree(results_files_path)\n",
    "\n",
    "os.makedirs(results_files_path)\n",
    "if draw_plot:\n",
    "    os.makedirs(results_files_path + \"/classes\")\n",
    "if show_animation:\n",
    "    os.makedirs(results_files_path + \"/images\")\n",
    "    os.makedirs(results_files_path + \"/images/single_predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拿到ground_truth\n",
    "\n",
    "有三个任务：\n",
    "\n",
    "    1、按照图片保存ground_truth信息。\n",
    "    2、保存每个类别出现过几个ground_truth\n",
    "    3、保存每个类别出现在图片中的次数（一个图片中出现多个类别A也记为1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Ground-Truth\n",
    "     Load each of the ground-truth files into a temporary \".json\" file.\n",
    "     Create a list of all the class names present in the ground-truth (gt_classes).\n",
    "\"\"\"\n",
    "# get a list with the ground-truth files\n",
    "# 格式是['ground-truth\\\\2007_000027.txt', 'ground-truth\\\\2007_000032.txt',...]这样的\n",
    "ground_truth_files_list = glob.glob('ground-truth/*.txt')\n",
    "if len(ground_truth_files_list) == 0:\n",
    "    error(\"Error: No ground-truth files found!\")\n",
    "ground_truth_files_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with counter per class\n",
    "gt_counter_per_class = {}\n",
    "counter_images_per_class = {}\n",
    "\n",
    "'''\n",
    "    这里明确一下：\n",
    "        1、一个*.txt文件对应一张*.jpg的图。\n",
    "        2、一个*.txt文件中，会包含多个gt\n",
    "'''\n",
    "for txt_file in ground_truth_files_list:\n",
    "    '''\n",
    "        从ground-truth\\\\2007_000027.txt\n",
    "        拿到了：2007_000027\n",
    "    '''\n",
    "    file_id = txt_file.split(\".txt\",1)[0]\n",
    "    file_id = os.path.basename(os.path.normpath(file_id))\n",
    "    # 看看对应的图片是否存在\n",
    "    # check if there is a correspondent predicted objects file\n",
    "    if not os.path.exists('predicted/' + file_id + \".txt\"):\n",
    "        error_msg = \"Error. File not found: predicted/\" + file_id + \".txt\\n\"\n",
    "        error_msg += \"(You can avoid this error message by running extra/intersect-gt-and-pred.py)\"\n",
    "        error(error_msg)\n",
    "\n",
    "    # 拿到文件中的每行\n",
    "    lines_list = file_lines_to_list(txt_file)\n",
    "    # create ground-truth dictionary\n",
    "    bounding_boxes = []\n",
    "    is_difficult = False\n",
    "    already_seen_classes = []\n",
    "    '''\n",
    "        1、对于这个图中的每个gt，拿到对应的类别和位置\n",
    "        2、统计每个类别有几个gt。\n",
    "        3、统计每个图片有几种gt。\n",
    "    '''\n",
    "    for line in lines_list:\n",
    "        try:\n",
    "            if \"difficult\" in line:\n",
    "                    class_name, left, top, right, bottom, _difficult = line.split()\n",
    "                    is_difficult = True\n",
    "            else:\n",
    "                    class_name, left, top, right, bottom = line.split()\n",
    "        except ValueError:\n",
    "            error_msg = \"Error: File \" + txt_file + \" in the wrong format.\\n\"\n",
    "            error_msg += \" Expected: <class_name> <left> <top> <right> <bottom> ['difficult']\\n\"\n",
    "            error_msg += \" Received: \" + line\n",
    "            error_msg += \"\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n\"\n",
    "            error_msg += \"by running the script \\\"remove_space.py\\\" or \\\"rename_class.py\\\" in the \\\"extra/\\\" folder.\"\n",
    "            error(error_msg)\n",
    "        # check if class is in the ignore list, if yes skip\n",
    "        if class_name in []:\n",
    "            continue\n",
    "        bbox = left + \" \" + top + \" \" + right + \" \" +bottom\n",
    "        if is_difficult:\n",
    "                bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False, \"difficult\":True})\n",
    "                is_difficult = False\n",
    "        else:\n",
    "                bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
    "\n",
    "                '''\n",
    "                记录每个种类出现过几次\n",
    "                '''\n",
    "                # count that object\n",
    "                if class_name in gt_counter_per_class:\n",
    "                    gt_counter_per_class[class_name] += 1\n",
    "                else:\n",
    "                    # if class didn't exist yet\n",
    "                    gt_counter_per_class[class_name] = 1\n",
    "                '''\n",
    "                    这个感觉和上面的一样啊…\n",
    "                '''\n",
    "                if class_name not in already_seen_classes:\n",
    "                    if class_name in counter_images_per_class:\n",
    "                        counter_images_per_class[class_name] += 1\n",
    "                    else:\n",
    "                        # if class didn't exist yet\n",
    "                        counter_images_per_class[class_name] = 1\n",
    "                    already_seen_classes.append(class_name)\n",
    "\n",
    "\n",
    "    # dump bounding_boxes into a \".json\" file\n",
    "    with open(TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\", 'w') as outfile:\n",
    "        json.dump(bounding_boxes, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n下面是每一个被存储起来的gt的样子，存储方式：\\n    1、按照图片被存储，每张图片一个文件。\\n    2、保存内容有gt的位置，类别，是否被使用\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes\n",
    "'''\n",
    "下面是每一个被存储起来的gt的样子，存储方式：\n",
    "    1、按照图片被存储，每张图片一个文件。\n",
    "    2、保存内容有gt的位置，类别，是否被使用\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pictureframe': 24, 'heater': 13, 'pottedplant': 29, 'book': 33, 'coffeetable': 22, 'tvmonitor': 20, 'bookcase': 7, 'doll': 8, 'vase': 12, 'shelf': 6, 'windowblind': 17, 'door': 29, 'wastecontainer': 11, 'nightstand': 7, 'sofa': 21, 'pillow': 45, 'chair': 106, 'diningtable': 47, 'remote': 8, 'cup': 36, 'cabinetry': 52, 'tap': 18, 'tincan': 28, 'countertop': 21, 'sink': 14, 'person': 7, 'bed': 8, 'bottle': 11, 'bowl': 15, 'backpack': 11}\n",
      "{'pictureframe': 19, 'heater': 13, 'pottedplant': 22, 'book': 7, 'coffeetable': 16, 'tvmonitor': 16, 'bookcase': 7, 'doll': 8, 'vase': 10, 'shelf': 6, 'windowblind': 15, 'door': 28, 'wastecontainer': 11, 'nightstand': 5, 'sofa': 21, 'pillow': 21, 'chair': 46, 'diningtable': 38, 'remote': 8, 'cup': 21, 'cabinetry': 45, 'tap': 18, 'tincan': 18, 'countertop': 21, 'sink': 14, 'person': 6, 'bed': 8, 'bottle': 10, 'bowl': 12, 'backpack': 7}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "记录的是每个类别总共有几个gt\n",
    "'''\n",
    "print(gt_counter_per_class)\n",
    "\n",
    "'''\n",
    "每张图class去重之后，再记录\n",
    "'''\n",
    "print(counter_images_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    拿到所有出现过的class类别\n",
    "    并排序，取长度啥的\n",
    "'''\n",
    "gt_classes = list(gt_counter_per_class.keys())\n",
    "# let's sort the classes alphabetically\n",
    "gt_classes = sorted(gt_classes)\n",
    "n_classes = len(gt_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拿到prediction信息\n",
    "\n",
    "任务：\n",
    "\n",
    "    1、按照不同的类别，保存各个预测测bbox。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测的信息\n",
    "# get a list with the predicted files\n",
    "predicted_files_list = glob.glob('predicted/*.txt')\n",
    "predicted_files_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    这里很有意思，不是读取每一个predict文件，而是按照类别来\n",
    "    1、预测文件按照类别进行保存\n",
    "'''\n",
    "\n",
    "# 对于每一个类别\n",
    "for class_index, class_name in enumerate(gt_classes):\n",
    "    bounding_boxes = []\n",
    "    # 遍历每一个预测文件\n",
    "    for txt_file in predicted_files_list:\n",
    "        #print(txt_file)\n",
    "        # the first time it checks if all the corresponding ground-truth files exist\n",
    "        file_id = txt_file.split(\".txt\",1)[0]\n",
    "        file_id = os.path.basename(os.path.normpath(file_id))\n",
    "        # 第一次进来检查一下\n",
    "        if class_index == 0:\n",
    "            if not os.path.exists('ground-truth/' + file_id + \".txt\"):\n",
    "                error_msg = \"Error. File not found: ground-truth/\" +    file_id + \".txt\\n\"\n",
    "                error_msg += \"(You can avoid this error message by running extra/intersect-gt-and-pred.py)\"\n",
    "                error(error_msg)\n",
    "\n",
    "        lines = file_lines_to_list(txt_file)\n",
    "        # 对于文件中的每一行\n",
    "        # 这些行应该是已经做过NMS的\n",
    "        for line in lines:\n",
    "            # 拿到分类,置信度和坐标信息\n",
    "            try:\n",
    "                tmp_class_name, confidence, left, top, right, bottom = line.split()\n",
    "            except ValueError:\n",
    "                error_msg = \"Error: File \" + txt_file + \" in the wrong format.\\n\"\n",
    "                error_msg += \" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n\"\n",
    "                error_msg += \" Received: \" + line\n",
    "                error(error_msg)\n",
    "            # 如果是当前类别的bbox，那么保存\n",
    "            if tmp_class_name == class_name:\n",
    "                #print(\"match\")\n",
    "                bbox = left + \" \" + top + \" \" + right + \" \" +bottom\n",
    "                bounding_boxes.append({\"confidence\":confidence, \"file_id\":file_id, \"bbox\":bbox})\n",
    "                #print(bounding_boxes)\n",
    "\n",
    "    # 按照置信度降序保存\n",
    "    # sort predictions by decreasing confidence\n",
    "    bounding_boxes.sort(key=lambda x:float(x['confidence']), reverse=True)\n",
    "    with open(TEMP_FILES_PATH + \"/\" + class_name + \"_predictions.json\", 'w') as outfile:\n",
    "        json.dump(bounding_boxes, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backpack [{'confidence': '0.552314', 'file_id': '2007_000837', 'bbox': '45 216 220 313'}, {'confidence': '0.552256', 'file_id': '2007_000648', 'bbox': '126 249 332 455'}, {'confidence': '0.411606', 'file_id': '2007_000762', 'bbox': '374 231 568 466'}, {'confidence': '0.374395', 'file_id': '2007_000661', 'bbox': '287 230 466 470'}, {'confidence': '0.350580', 'file_id': '2007_000822', 'bbox': '20 133 121 333'}] 5\n",
      "[1, 0, 1, 1, 0] [0, 1, 0, 0, 1]\n",
      "backpack 11\n",
      "[1, 1, 2, 3, 3] [0, 1, 1, 1, 2]\n",
      "recal: [0.09090909090909091, 0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.2727272727272727]\n",
      "prec [1.0, 0.5, 0.6666666666666666, 0.75, 0.6]\n",
      "22.73% = backpack AP \n",
      "bed [{'confidence': '0.936491', 'file_id': '2007_000452', 'bbox': '1 88 599 473'}, {'confidence': '0.930039', 'file_id': '2007_000837', 'bbox': '12 91 586 467'}, {'confidence': '0.870608', 'file_id': '2007_000515', 'bbox': '0 92 442 448'}, {'confidence': '0.848061', 'file_id': '2007_000799', 'bbox': '5 96 478 456'}, {'confidence': '0.710099', 'file_id': '2007_000123', 'bbox': '19 90 143 182'}, {'confidence': '0.438210', 'file_id': '2007_000645', 'bbox': '0 78 140 184'}, {'confidence': '0.363359', 'file_id': '2007_000123', 'bbox': '1 63 133 215'}, {'confidence': '0.263161', 'file_id': '2007_000636', 'bbox': '33 57 117 194'}] 8\n",
      "[1, 1, 1, 1, 1, 1, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "bed 8\n",
      "[1, 2, 3, 4, 5, 6, 7, 8] [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "recal: [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
      "prec [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mAP = 0.76%\n"
     ]
    }
   ],
   "source": [
    "sum_AP = 0.0\n",
    "ap_dictionary = {}\n",
    "lamr_dictionary = {}\n",
    "# open file to store the results\n",
    "with open(results_files_path + \"/results.txt\", 'w') as results_file:\n",
    "    results_file.write(\"# AP and precision/recall per class\\n\")\n",
    "    count_true_positives = {}\n",
    "    # 对于每个类别\n",
    "    for class_index, class_name in enumerate(gt_classes):\n",
    "        count_true_positives[class_name] = 0\n",
    "        \"\"\"\n",
    "         Load predictions of that class\n",
    "         拿到这个类别的预测结果：\n",
    "            1、这个预测结果都是属于这个类别的。\n",
    "            2、这个预测结果是跨图片的。\n",
    "        \"\"\"\n",
    "        predictions_file = TEMP_FILES_PATH + \"/\" + class_name + \"_predictions.json\"\n",
    "        predictions_data = json.load(open(predictions_file)) \n",
    "\n",
    "        \"\"\"\n",
    "         Assign predictions to ground truth objects\n",
    "        \"\"\"\n",
    "        nd = len(predictions_data)\n",
    "        tp = [0] * nd # creates an array of zeros of size nd\n",
    "        fp = [0] * nd\n",
    "              \n",
    "        '''\n",
    "        对于这个类的每一个预测结果：\n",
    "            1、拿到对应图片的名字\n",
    "            2、拿到这个图片对应的gt_file\n",
    "                \n",
    "        '''\n",
    "        for idx, prediction in enumerate(predictions_data):\n",
    "            # 拿到对应图片的名字\n",
    "            file_id = prediction[\"file_id\"]\n",
    "           \n",
    "            # assign prediction to ground truth object if any\n",
    "            # open ground-truth with that file_id\n",
    "            # 拿到这个图片对应的gt_file\n",
    "            gt_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
    "            ground_truth_data = json.load(open(gt_file))\n",
    "            ovmax = -1\n",
    "            gt_match = -1\n",
    "            # load prediction bounding-box\n",
    "            # 拿到这个bbox的坐标\n",
    "            bb = [ float(x) for x in prediction[\"bbox\"].split() ]\n",
    "            # 对于这个gt_file中的每个gt\n",
    "            temp_gtd = copy.deepcopy(ground_truth_data)\n",
    "            for obj in temp_gtd:\n",
    "                # look for a class_name match\n",
    "                # 如果类别相同，计算IOU\n",
    "                if obj[\"class_name\"] == class_name:\n",
    "                    bbgt = [ float(x) for x in obj[\"bbox\"].split() ]\n",
    "                    bi = [max(bb[0],bbgt[0]),\n",
    "                          max(bb[1],bbgt[1]),\n",
    "                          min(bb[2],bbgt[2]),\n",
    "                          min(bb[3],bbgt[3])]\n",
    "                    iw = bi[2] - bi[0] + 1\n",
    "                    ih = bi[3] - bi[1] + 1\n",
    "                    # 计算IOU\n",
    "                    if iw > 0 and ih > 0:\n",
    "                        # compute overlap (IoU) = area of intersection / area of union\n",
    "                        ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n",
    "                                        + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n",
    "                        ov = iw * ih / ua\n",
    "                        # 计算这个bbox和哪个类别相同的gt的IOU最高。\n",
    "                        if ov > ovmax:\n",
    "                            ovmax = ov\n",
    "                            gt_match = obj\n",
    "\n",
    "            # assign prediction as true positive/don't care/false positive\n",
    "           \n",
    "            # set minimum overlap\n",
    "            min_overlap = MINOVERLAP # 0.5\n",
    "           \n",
    "            '''\n",
    "            来，看看什么时候才算一个TP:\n",
    "            1、对于某一个类别A；\n",
    "            2、拿到这个类别A的所有预测框。\n",
    "            3、对于某一个预测框b；\n",
    "            4、拿到它对应的图片p；\n",
    "            5、拿到这个图片上所有类别为A的gts。\n",
    "            6、计算b和gts中哪个gt的IOU最大。\n",
    "            7、如果IOU超过了设定的阈值，那么算一个tp\n",
    "            8、如果IOU没超过，算一个FP\n",
    "            9、如果IOU超过了，但是这个gt被用过了，算一个FP\n",
    "            '''\n",
    "            if ovmax >= min_overlap:\n",
    "                if \"difficult\" not in gt_match:\n",
    "                        # 这个gt还没有用过\n",
    "                        if not bool(gt_match[\"used\"]):\n",
    "                            '''\n",
    "                                1、这里的idx表示的是这个类别的第几个预测框\n",
    "                                2、因此，这里会有len(gt_class)个tp，每个tp的长度都不一样。\n",
    "                                3、fp同理。\n",
    "                            '''\n",
    "                            # true positive\n",
    "                            tp[idx] = 1\n",
    "                            gt_match[\"used\"] = True\n",
    "                            count_true_positives[class_name] += 1\n",
    "                            # update the \".json\" file\n",
    "                            with open(gt_file, 'w') as f:\n",
    "                                    f.write(json.dumps(ground_truth_data))\n",
    "                            if show_animation:\n",
    "                                status = \"MATCH!\"\n",
    "                        else:\n",
    "                            # false positive (multiple detection)\n",
    "                            fp[idx] = 1\n",
    "                            if show_animation:\n",
    "                                status = \"REPEATED MATCH!\"\n",
    "            else:\n",
    "                # false positive\n",
    "                fp[idx] = 1\n",
    "                if ovmax > 0:\n",
    "                    status = \"INSUFFICIENT OVERLAP\"\n",
    "\n",
    "        print(class_name,predictions_data,len(predictions_data))\n",
    "        print(tp,fp)\n",
    "                    \n",
    "        '''\n",
    "            其实这里是这样：\n",
    "                1、观察predictions_data，即同一个类别的预测框。它是按照属于这个类别的置信度进行降序排列的。\n",
    "                2、那么这里的相加，表示，当我的阈值不断放松的时候，对于那些已经度过了IOU关卡的预测bbox而言，会有多少bbox能够被收容。\n",
    "                3、比如过了属于backpack类别的预测bbox只有5个，且他们过了IOU的tp是[1,0,1,1,0],表示出了第1个和第4个与gt的iou太小被删除，\n",
    "                   其他都在。且从0到4，他们是属于backpack类别的置信度是降序排列的。\n",
    "                4、然后从头累加，得到[1,1,2,3,3],表示当阈值设置的最大，只有1个bbox是tp；当阈值设置第次大，也只有1个bbox是tp；\n",
    "                   当阈值再小，就有2个bbox是tp了…以此类推。\n",
    "                5、fp的操作也一样。\n",
    "                6、这样，就完成了这个类别的precision-recall曲线\n",
    "        '''\n",
    "        cumsum = 0\n",
    "        for idx, val in enumerate(fp):\n",
    "            fp[idx] += cumsum\n",
    "            cumsum += val\n",
    "        cumsum = 0\n",
    "        for idx, val in enumerate(tp):\n",
    "            tp[idx] += cumsum\n",
    "            cumsum += val\n",
    "            \n",
    "        \n",
    "        print(class_name,gt_counter_per_class[class_name])\n",
    "        #print(tp)\n",
    "        rec = tp[:]\n",
    "        for idx, val in enumerate(tp):\n",
    "            # 是对每一个图？\n",
    "            rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
    "        #print(rec)\n",
    "        prec = tp[:]\n",
    "        for idx, val in enumerate(tp):\n",
    "            # 是对每一个图？\n",
    "            prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
    "        #print(prec)\n",
    "        \n",
    "        print(tp,fp)\n",
    "\n",
    "        print('recal:',rec)\n",
    "        print('prec',prec)\n",
    "\n",
    "\n",
    "        '''\n",
    "            1、这个是算面积的函数\n",
    "            利用的是precision monotonically decreasing方法。\n",
    "            2、具体而言，对于recall为r时刻的precision，我们将其设置为：\n",
    "                2.1 取得所有recall为r',满足r'>r时刻的precision。\n",
    "                2.2 将recall为r时刻的precision，设置为2.1中的最大值。\n",
    "            3、实际操作上，对于precision：pre = [1.0, 0.5, 0.6666666666666666, 0.75, 0.6]\n",
    "               我们认为precision逐渐变小，是由于阈值逐渐变小\n",
    "               而阈值逐渐变小，表示recall逐渐变大。\n",
    "            4、所以我们默认pre这个list后面的对应的recall值，大于前面的。\n",
    "            5、那么我们就只要从后往前，累计求极大就好了。\n",
    "            \n",
    "        '''\n",
    "        ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "        \n",
    "        if class_index==1:\n",
    "            break\n",
    "        \n",
    "        sum_AP += ap\n",
    "        text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
    "        \"\"\"\n",
    "         Write to results.txt\n",
    "        \"\"\"\n",
    "        rounded_prec = [ '%.2f' % elem for elem in prec ]\n",
    "        rounded_rec = [ '%.2f' % elem for elem in rec ]\n",
    "        results_file.write(text + \"\\n Precision: \" + str(rounded_prec) + \"\\n Recall :\" + str(rounded_rec) + \"\\n\\n\")\n",
    "        print(text)\n",
    "        ap_dictionary[class_name] = ap\n",
    "\n",
    "        n_images = counter_images_per_class[class_name]\n",
    "        lamr, mr, fppi = log_average_miss_rate(np.array(rec), np.array(fp), n_images)\n",
    "        lamr_dictionary[class_name] = lamr\n",
    "\n",
    "       \n",
    "\n",
    "  \n",
    "\n",
    "    results_file.write(\"\\n# mAP of all classes\\n\")\n",
    "    mAP = sum_AP / n_classes\n",
    "    text = \"mAP = {0:.2f}%\".format(mAP*100)\n",
    "    results_file.write(text + \"\\n\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_ap(rec, prec):\n",
    "    \n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    \n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    print(mpre)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "        print(mpre[i], mpre[i+1])\n",
    "        mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    print(mpre)\n",
    "\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    print(mrec)\n",
    "    print(i_list)\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap, mrec, mpre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.5, 0.6666666666666666, 0.75, 0.6, 0.0]\n",
      "0.6 0.0\n",
      "0.75 0.6\n",
      "0.6666666666666666 0.75\n",
      "0.5 0.75\n",
      "1.0 0.75\n",
      "0.0 1.0\n",
      "[1.0, 1.0, 0.75, 0.75, 0.75, 0.6, 0.0]\n",
      "[0.0, 0.09090909090909091, 0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.2727272727272727, 1.0]\n",
      "[1, 3, 4, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22727272727272724,\n",
       " [0.0,\n",
       "  0.09090909090909091,\n",
       "  0.09090909090909091,\n",
       "  0.18181818181818182,\n",
       "  0.2727272727272727,\n",
       "  0.2727272727272727,\n",
       "  1.0],\n",
       " [1.0, 1.0, 0.75, 0.75, 0.75, 0.6, 0.0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = [0.09090909090909091, 0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.2727272727272727]\n",
    "pre = [1.0, 0.5, 0.6666666666666666, 0.75, 0.6]\n",
    "voc_ap(rec,pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
