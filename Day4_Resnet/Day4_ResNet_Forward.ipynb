{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标\n",
    "\n",
    "    1、复写一遍Tensorflow定义的resnet结构。\n",
    "    \n",
    "resnet_152:\n",
    "\n",
    "    1、一层 7*7卷积\n",
    "    2：【1x1x64,3x3x64,1x1x256】x3 卷积\n",
    "    3：【1x1x128,3x3x128,1x1x512】x8 卷积\n",
    "    4：【1x1x256,3x3x256,1x1x1024】x36 卷积\n",
    "    5：【1x1x512,3x3x512,1x1x2048】x3 卷积\n",
    "    6：一层 FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "LOG_DIR='./logs/forward/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1、class Block(object)中，object表示Block的父类是object。\n",
    "2、collections.namedtuple()本质上是一个tuple的子类，\n",
    "   但是可以像类一样操作。比如定义名为Block的块，属性有下面三个。\n",
    "3、下面的做法就是定义了一个继承自namedtuple的子类，Block\n",
    "'''\n",
    "class Block(collections.namedtuple('Block',\n",
    "                                   ['scope','unit_fn','args'])):\n",
    "        \n",
    "    '''\n",
    "  使用collections.namedtuple设计ResNet基本模块组的name tuple，\n",
    "  并用它创建Block的类\n",
    "  只包含数据结构，不包含具体方法。\n",
    "  \n",
    "  定义一个典型的Block，需要输入三个参数：\n",
    "  scope：Block的名称\n",
    "  unit_fn：ResNet V2中的残差学习单元 \n",
    "  args：Block的args。\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义降采样方法\n",
    "\n",
    "    注意，这里虽然使用了maxpool，\n",
    "    但是本质上是用了降采样，而不是池化技术来进行尺寸缩减的。\n",
    "'''\n",
    "def subsample(inputs,factor,scope=None):\n",
    "    '''\n",
    "    inputs:输入tensor\n",
    "    factor：采样因子（理解为隔多少个像素取一个样本\n",
    "    scope：命名空间\n",
    "    '''\n",
    "    if inputs == 1:\n",
    "        return inputs\n",
    "    else:\n",
    "        return slim.max_pool2d(inputs,[1,1],stride=factor,scope=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积层\n",
    "    目的：\n",
    "        让输出feature-map的尺寸和输入的一样。\n",
    "    因此：\n",
    "        1、当stride=1时，直接padding=’SAME‘\n",
    "        2、当stride不为1时，需要自己来做padding\n",
    "'''\n",
    "def conv2_same(inputs,num_outputs,kernel_size,stride,scope=None):\n",
    "    if stride == 1:\n",
    "        return slim.conv2d(inputs,num_outputs,kernel_size,stride=1,\n",
    "                           padding='SAME',scope=scope)\n",
    "    else:\n",
    "        '''\n",
    "            inputs有4维：batch，h,w,channel,我们要在h和w上padding\n",
    "            pad中的第一个[0，0],表示我们在batch这一维，开头不补，结尾不补。\n",
    "            其他同理\n",
    "            呃，这里其实没有处理吧…感觉没变化的啊\n",
    "            试验了，是没变化的…不明白为什么要怎么做\n",
    "        '''\n",
    "        pad_total = kernel_size-1\n",
    "        pad_beg = pad_total // 2\n",
    "        pad_end = pad_total-pad_beg\n",
    "        inputs = tf.pad(inputs,[[0,0],[pad_beg,pad_end],\n",
    "                                [pad_beg,pad_end],[0,0]])\n",
    "        return slim.conv2d(inputs,num_outputs,kernel_size,stride=stride,\n",
    "                           padding='VALID',scope=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义堆叠block的函数\n",
    "只有用@slim.add_arg_scope修饰过的参数才能用 with slim.arg_scope()来设置参数\n",
    "'''\n",
    "@slim.add_arg_scope\n",
    "def stack_blocks_dense(net,blocks,outputs_collections=None):\n",
    "    '''\n",
    "  Args:\n",
    "    net: A `Tensor` of size [batch, height, width, channels].输入。\n",
    "    blocks: 是之前定义的Block的class的列表。\n",
    "    outputs_collections: 收集各个end_points的collections。\n",
    "  Returns:\n",
    "    net: Output tensor \n",
    "    '''\n",
    "    \n",
    "    for block in blocks:\n",
    "        with tf.variable_scope(block.scope,'block',[net]) as sc:\n",
    "            '''\n",
    "                1、block.args是形如这样的list\n",
    "                    [(256, 64, 1), (256, 64, 1), (256, 64, 2)]\n",
    "                2、unit指的就是里面的每一个(256,64,1)这样的\n",
    "                3、block.unit_fn在这里指的就是bottleneck这个函数\n",
    "            '''\n",
    "            for i ,unit in enumerate(block.args):\n",
    "                with tf.variable_scope('unit_%d'%(i+1),values=[net]):\n",
    "                    unit_depth,unit_depth_bottleneck,unit_strid = unit\n",
    "                    '''\n",
    "                        虽然这里的block.unit_fn没有定义outputs_collections\n",
    "                        但是之前用arg_scope定义过了，所以还是有值的。\n",
    "                    '''\n",
    "                    net = block.unit_fn(net,\n",
    "                                        depth=unit_depth,\n",
    "                                        depth_bottleneck = unit_depth_bottleneck,\n",
    "                                        stride = unit_strid)\n",
    "            net = slim.utils.collect_named_outputs(outputs_collections,sc.name,net)\n",
    "            print(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "创建arg_scope,用来定义模型超参数的默认值\n",
    "'''\n",
    "def resnet_arg_scope(is_training=True,\n",
    "                     weight_decay = 0.0001,\n",
    "                     batch_norm_decay = 0.997,\n",
    "                     batch_norm_epsilon = 1e-5,\n",
    "                     batch_norm_scale = True):\n",
    "    batch_norm_params = {\n",
    "        'is_training':is_training,\n",
    "        'decay':batch_norm_decay,\n",
    "        'epsilon':batch_norm_epsilon,\n",
    "        'scale':batch_norm_scale,\n",
    "        'updates_collections':tf.GraphKeys.UPDATE_OPS\n",
    "    }\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                        weights_regularizer = slim.l2_regularizer(weight_decay),\n",
    "                        weights_initializer = slim.variance_scaling_initializer(),\n",
    "                        activation_fn = tf.nn.relu,\n",
    "                        normalizer_fn = slim.batch_norm,\n",
    "                        normalizer_params = batch_norm_params):\n",
    "        with slim.arg_scope([slim.batch_norm],**batch_norm_params):\n",
    "            with slim.arg_scope([slim.max_pool2d],padding='SAME') as arg_sc:\n",
    "                return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "核心：定义一个残差单元的地方\n",
    "\n",
    "关于tf.variable_scope中的values解释：https://stackoverflow.com/questions/40164583/tensorflows-tensorflow-variable-scope-values-parameter-meaning\n",
    "大意就是能够是来自不同graph的variable在一起被操作吧\n",
    "\n",
    "'''\n",
    "@slim.add_arg_scope\n",
    "def bottleneck(inputs,depth,depth_bottleneck,stride,\n",
    "               outputs_collections=None,scope=None):\n",
    "    '''\n",
    "    Args:\n",
    "    inputs: A tensor of size [batch, height, width, channels].\n",
    "    depth、depth_bottleneck，stride三个参数是前面blocks类中\n",
    "        depth表示这个残差块最终输出channel\n",
    "        depth_bottleneck表示这个残差块中间的卷积核输出channel\n",
    "        stride表示卷积步长\n",
    "    outputs_collections: 是收集end_points的collection\n",
    "    scope: 是这个unit的名称。\n",
    "    '''\n",
    "    with tf.variable_scope(scope,'bottleneck_v2',[inputs]) as sc:\n",
    "        depth_in = slim.utils.last_dimension(inputs.get_shape(),\n",
    "                                             min_rank=4)\n",
    "        '''\n",
    "            1、回忆resnet的顺序，\n",
    "                第一条路：先normal，再激活，然后再卷积\n",
    "                第二条路：shortcuts\n",
    "            \n",
    "        '''\n",
    "        preact = slim.batch_norm(inputs,activation_fn=tf.nn.relu,scope='preact')\n",
    "        \n",
    "        \n",
    "        '''\n",
    "            下面是short_cuts部分\n",
    "        '''\n",
    "        \n",
    "        if depth == depth_in:\n",
    "            shortcut = subsample(inputs,stride,'shortcut')\n",
    "        else:\n",
    "            shortcut = slim.conv2d(preact,depth,[1,1],stride=stride,\n",
    "                                   normalizer_fn=None,activation_fn=None,\n",
    "                                   scope='shortcut')\n",
    "            \n",
    "        '''\n",
    "            下面是非short_cuts部分\n",
    "        '''\n",
    "        residual = slim.conv2d(preact,depth_bottleneck,[1,1],\n",
    "                               stride=1,scope='conv1')\n",
    "        \n",
    "        # 就只有这里stride可能是2，所以需要特殊写一个conv方法，来处理\n",
    "        residual = conv2_same(residual,depth_bottleneck,3,stride,\n",
    "                              scope='conv2')\n",
    "        \n",
    "        residual = slim.conv2d(residual,depth,[1,1],stride=1,\n",
    "                               normalizer_fn=None,\n",
    "                               activation_fn=None,scope='conv3')\n",
    "        \n",
    "        output = shortcut+residual\n",
    "        '''\n",
    "            这里大致意思是\n",
    "                    把output这个tensor\n",
    "                    以别名sc.name的方式\n",
    "                    加入到名字为outputs_collections中去\n",
    "                然后再返回output这个tensor\n",
    "        '''\n",
    "        return slim.utils.collect_named_outputs(outputs_collections,\n",
    "                                                sc.name,output)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义resnet-v2的主函数\n",
    "'''\n",
    "def resnet_v2(inputs,blocks,num_classes=None,\n",
    "              global_pool=True,include_root_block=True,\n",
    "              reuse=None,scope=None):\n",
    "    '''\n",
    "        之前试着把这个reuse=reuse删了，发现也没什么问题，表明其实这里并没有需要reuse的参数\n",
    "        \n",
    "    '''\n",
    "    with tf.variable_scope(scope,'resnet_v2',[inputs],reuse=reuse) as sc:\n",
    "        '''\n",
    "            拿到了一个字符串，作为namescope\n",
    "        '''\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        \n",
    "        '''\n",
    "            bottleneck和stack_blocks_dense\n",
    "            都是我们刚刚定义的方法\n",
    "            定义他们的name_scope都是end_points_collection\n",
    "        '''\n",
    "        with slim.arg_scope([slim.conv2d,bottleneck,stack_blocks_dense],\n",
    "                            outputs_collections = end_points_collection):\n",
    "            net = inputs\n",
    "            if include_root_block:\n",
    "                '''\n",
    "                    这里做的是ResNet中的第一层，7x7的卷积\n",
    "                '''\n",
    "                with slim.arg_scope([slim.conv2d],\n",
    "                                    activation_fn=None,\n",
    "                                    normalizer_fn=None):\n",
    "                    net = conv2_same(net,64,7,stride = 2,scope='conv1')\n",
    "                net = slim.max_pool2d(net,[3,3],stride=2,scope='pool1')\n",
    "            '''\n",
    "                进行block的堆叠\n",
    "            '''\n",
    "            net = stack_blocks_dense(net,blocks)\n",
    "            '''\n",
    "                对堆叠完的block进行BN层操作\n",
    "            '''\n",
    "            net = slim.batch_norm(net,activation_fn=tf.nn.relu,scope='postnorm')\n",
    "            \n",
    "            '''\n",
    "                全局平均池化，【1，2】指的是net的维度\n",
    "            '''\n",
    "            if global_pool:\n",
    "                net = tf.reduce_mean(net,[1,2],name='pool5',keep_dims=True)\n",
    "            '''\n",
    "                用1*1压缩/放大全局平均池化结果，作为最后的输出\n",
    "            '''\n",
    "            if num_classes is not None:\n",
    "                net = slim.conv2d(net,num_classes,[1,1],activation_fn=None,normalizer_fn=None,scope='logits')\n",
    "            \n",
    "            '''\n",
    "                根据名字end_point_collection将所有变量转换为dict\n",
    "            '''\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            '''\n",
    "                并且把最终结果也加进去\n",
    "            '''\n",
    "            if num_classes is not None:\n",
    "                end_points['predictions'] = slim.softmax(net,scope='predictions')\n",
    "            return net,end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2_152(inputs,num_classes=None,\n",
    "                  global_pool=True,reuse=None,\n",
    "                  scope='resnet_v2_152'):\n",
    "    blocks = [\n",
    "            Block('block1',bottleneck,[(256,64,1)]*2 + [(256,64,2)]),\n",
    "            Block('block2',bottleneck,[(512,128,1)]*7 + [(512,128,2)]),\n",
    "            Block('block3',bottleneck,[(1024,256,1)]*35+[(1024,256,2)]),\n",
    "            Block('block4',bottleneck,[(2048,512,1)]*3)\n",
    "             ]\n",
    "    return resnet_v2(inputs,blocks,num_classes,global_pool,\n",
    "                     include_root_block=True,reuse=reuse,scope=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试函数\n",
    "'''\n",
    "def time_tensorflow_run(session,target,info_string):\n",
    "    num_steps_burn_in = 10\n",
    "    total_duration = 0.0\n",
    "    total_duration_squared = 0.0\n",
    "    \n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time =time.time()\n",
    "        _ = sess.run(target)\n",
    "        duration = time.time()-start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            if not i%10:\n",
    "                print('%s: step %d, duration = %.3f' %\n",
    "                       (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration+=duration\n",
    "            total_duration_squared+=duration**2\n",
    "    mn = total_duration / num_batches\n",
    "    vr = total_duration_squared / num_batches - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "           (datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet_v2_152/block1/unit_3/bottleneck_v2/add:0\", shape=(32, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"resnet_v2_152/block2/unit_8/bottleneck_v2/add:0\", shape=(32, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"resnet_v2_152/block3/unit_36/bottleneck_v2/add:0\", shape=(32, 7, 7, 1024), dtype=float32)\n",
      "Tensor(\"resnet_v2_152/block4/unit_3/bottleneck_v2/add:0\", shape=(32, 7, 7, 2048), dtype=float32)\n",
      "2019-02-09 22:55:34.354135: step 0, duration = 9.683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-7b0de7f2fc94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtime_tensorflow_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-6bb4a89047fc>\u001b[0m in \u001b[0;36mtime_tensorflow_run\u001b[0;34m(session, target, info_string)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_steps_burn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_steps_burn_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 32\n",
    "height,width = 224,224\n",
    "inputs = tf.random_uniform(shape=(batch_size,height,width,3)\n",
    "                           ,dtype=tf.float32)\n",
    "\n",
    "with slim.arg_scope(resnet_arg_scope(is_training=False)):\n",
    "    net,end_points = resnet_v2_152(inputs,1000)\n",
    "    \n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf.summary.FileWriter(LOG_DIR,sess.graph)\n",
    "    num_batches =100\n",
    "    time_tensorflow_run(sess,net,'forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_v2_152/conv1 Tensor(\"resnet_v2_152/conv1/BiasAdd:0\", shape=(32, 112, 112, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for k in end_points.keys():\n",
    "    print(k,end_points[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
