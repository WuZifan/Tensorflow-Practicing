{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标：\n",
    "\n",
    "    1、利用resnet-50构建模型，并开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "\n",
    "    1、读取后缀为.h5的模型。\n",
    "    2、并区分训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH='./data/resnet50_dataset/train_signs.h5'\n",
    "TEST_DATA_PATH='./data/resnet50_dataset/test_signs.h5'\n",
    "LOG_DIR = './logs/'\n",
    "num_classes = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path,is_training=True):\n",
    "    \n",
    "    f = h5py.File(path,'r')\n",
    "    if is_training:\n",
    "        set_x = f['train_set_x'][:]\n",
    "        set_y = f['train_set_y'][:]\n",
    "    else:\n",
    "        set_x = f['test_set_x'][:]\n",
    "        set_y = f['test_set_y'][:]\n",
    "    \n",
    "    # data的尺寸，转换成resnet需要的尺寸\n",
    "    set_x = np.asarray([ cv2.resize(pic,(224,224)) for pic in set_x])\n",
    "    # label_one_hot\n",
    "    def to_one_hot(y,num_classes):\n",
    "        return np.eye(num_classes)[y.reshape(-1)]    \n",
    "    set_y = to_one_hot(set_y,num_classes)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return set_x,set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x,train_data_y = data_loader(TRAIN_DATA_PATH,is_training=True)\n",
    "test_data_x,test_data_y = data_loader(TEST_DATA_PATH,is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1080, 224, 224, 3), (1080, 6))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x.shape,train_data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "    1、尝试利用tf.slim来构建模型。\n",
    "    2、构建完毕后使用summary来查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_block(inputs,output_dim_list,stride):\n",
    "    # 如果输入输出的channel不同\n",
    "    input_channel=inputs.get_shape()[-1]\n",
    "    \n",
    "    if output_dim_list[2] == input_channel:\n",
    "        print('hehe')\n",
    "        short_cuts = slim.max_pool2d(inputs,(1,1),stride)\n",
    "    else:\n",
    "        print('haha')\n",
    "        # 这里只是为了提升channel而已\n",
    "        short_cuts = slim.conv2d(inputs,output_dim_list[2],(1,1),stride,padding='SAME')\n",
    "        short_cuts = slim.batch_norm(short_cuts)\n",
    "        \n",
    "    # 第一个conv2d的步长固定为1\n",
    "    res_path = slim.conv2d(inputs,output_dim_list[0],(1,1),stride=1,padding='SAME')\n",
    "    res_path = slim.batch_norm(res_path,activation_fn=tf.nn.relu)\n",
    "    \n",
    "    # 第二个conv2d的步长视情况而定\n",
    "    res_path = slim.conv2d(res_path,output_dim_list[1],(3,3),stride,padding='SAME')\n",
    "    res_path = slim.batch_norm(res_path,activation_fn=tf.nn.relu)\n",
    "    \n",
    "    # 第三个conv2d的步长还是1*1\n",
    "    res_path = slim.conv2d(res_path,output_dim_list[2],(1,1),stride=1,padding='SAME')\n",
    "    res_path = slim.batch_norm(res_path)\n",
    "    \n",
    "\n",
    "    res_output = res_path+short_cuts\n",
    "    \n",
    "    return tf.nn.relu(res_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n",
      "hehe\n",
      "hehe\n",
      "haha\n",
      "hehe\n",
      "hehe\n",
      "hehe\n",
      "haha\n",
      "hehe\n",
      "hehe\n",
      "hehe\n",
      "hehe\n",
      "hehe\n",
      "haha\n",
      "hehe\n",
      "hehe\n",
      "Tensor(\"resnet_50/softmax/Reshape_1:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_input = tf.placeholder(shape=(None,224,224,3),\n",
    "                         dtype=tf.float32,name='input_data')\n",
    "y_input = tf.placeholder(shape=(None,6),\n",
    "                         dtype=tf.float32,name='input_label')\n",
    "\n",
    "with tf.variable_scope('resnet_50'):\n",
    "    '''\n",
    "        block_conv1\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('Block_1'):\n",
    "        net = slim.conv2d(x_input,64,(7,7),stride=2,padding='SAME')\n",
    "        net = slim.max_pool2d(net,kernel_size=(3,3),stride=2,padding='SAME')\n",
    "    \n",
    "    '''\n",
    "        block_conv2\n",
    "    '''\n",
    "    with tf.variable_scope('Block_2'):\n",
    "        block_conv2_nums=3\n",
    "        for i in range(block_conv2_nums):\n",
    "            if i<block_conv2_nums-1:\n",
    "                temp_stride = 1\n",
    "            else:\n",
    "                temp_stride = 2\n",
    "            with tf.variable_scope('unit_%d'%(i+1)):\n",
    "                net = my_block(net,[64,64,256],temp_stride)\n",
    "    '''\n",
    "        block_conv3\n",
    "    '''\n",
    "    with tf.variable_scope('Block_3'):\n",
    "        block_conv3_nums=4\n",
    "        for i in range(block_conv3_nums):\n",
    "            if i<block_conv3_nums-1:\n",
    "                temp_stride = 1\n",
    "            else:\n",
    "                temp_stride = 2\n",
    "            with tf.variable_scope('unit_%d'%(i+1)):\n",
    "                net = my_block(net,[128,128,512],temp_stride)\n",
    "        \n",
    "    '''\n",
    "        block_conv4\n",
    "    '''\n",
    "    with tf.variable_scope('Block_4'):\n",
    "        block_conv4_nums=6\n",
    "        for i in range(block_conv4_nums):\n",
    "            if i<block_conv4_nums-1:\n",
    "                temp_stride = 1\n",
    "            else:\n",
    "                temp_stride = 2\n",
    "            with tf.variable_scope('unit_%d'%(i+1)):\n",
    "                net = my_block(net,[256,256,1024],temp_stride)\n",
    "    '''\n",
    "        block_conv5\n",
    "    '''\n",
    "    with tf.variable_scope('Block_5'):\n",
    "        block_conv5_nums=3\n",
    "        for i in range(block_conv5_nums):\n",
    "            temp_stride=1\n",
    "            with tf.variable_scope('unit_%d'%(i+1)):\n",
    "                net = my_block(net,[512,512,2048],temp_stride)\n",
    "    \n",
    "    '''\n",
    "        全局平均池化\n",
    "    '''\n",
    "    net = tf.reduce_mean(net,[1,2],keepdims=True)\n",
    "    \n",
    "    '''\n",
    "        接下来有两种选择：\n",
    "            1、用fc将平均池化结果变成我们想要的结果尺寸\n",
    "            2、用卷积将平均池化结果变成我们想要的结果尺寸\n",
    "            3、我们用卷积\n",
    "    '''\n",
    "    \n",
    "    net = slim.conv2d(net,num_classes,(1,1),1)\n",
    "    \n",
    "    net = tf.reshape(net,(-1,6))\n",
    "    \n",
    "    net_output = slim.softmax(net)\n",
    "    \n",
    "    print(net_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=net,labels=y_input)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=24\n",
    "epochs = 1000\n",
    "def get_batch_data(train_data_x,train_data_y,batch_size):\n",
    "    samples = len(train_data_y)\n",
    "    n_batch = samples//batch_size\n",
    "    i=0\n",
    "    while True:\n",
    "        i=i%n_batch\n",
    "        temp_x = train_data_x[i*batch_size:(i+1)*batch_size,:,:,:]\n",
    "        temp_y = train_data_y[i*batch_size:(i+1)*batch_size,:]\n",
    "        \n",
    "#         yield i\n",
    "        i=i+1\n",
    "        \n",
    "        yield temp_x,temp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_batch_data(train_data_x,train_data_y,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0 1.9524921\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-a637bb108573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf.summary.FileWriter(LOG_DIR,sess.graph)\n",
    "    num_batches= len(train_data_y)// batch_size\n",
    "    for i in range(epochs):\n",
    "        for j in range(num_batches):\n",
    "            batch_x,batch_y = next(generator)\n",
    "            feed_dict={x_input:batch_x,y_input:batch_y}\n",
    "            sess.run(optimizer,feed_dict=feed_dict)\n",
    "            \n",
    "            if j % 10 ==0:\n",
    "                loss_res =  sess.run(loss,feed_dict=feed_dict)\n",
    "                print(epochs,j,loss_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
